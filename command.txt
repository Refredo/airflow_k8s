Instruction:

build docker image
push in kind
deploy airflow using helm chart and airflow-values.yaml
configure variables (api_key, username for kafka, password for kafka)
deploy kafka using helm chart

sh create_k8s_cluster/create_kind_cluster.sh

docker build . -t extended_airflow:latest

kind load docker-image extended_airflow:latest

helm install airflow apache-airflow/airflow -f airflow-values.yaml

helm install kafka oci://registry-1.docker.io/bitnamicharts/kafka

helm install spark oci://registry-1.docker.io/bitnamicharts/spark
helm install spark-cluster oci://registry-1.docker.io/bitnamicharts/spark -f .\spark-config\spark-values.yaml

helm install elasticsearch oci://registry-1.docker.io/bitnamicharts/elasticsearch

-- kubectl port-forward --namespace default svc/elasticsearch 9200:9200

helm install kibana oci://registry-1.docker.io/bitnamicharts/kibana --set elasticsearch.hosts[0]=elasticsearch.default.svc.cluster.local --set elasticsearch.port=9200

kubectl port-forward svc/kibana 8080:5601

helm show values oci://registry-1.docker.io/bitnamicharts/kibana > kibana-values.yaml 



helm upgrade elasticsearch oci://registry-1.docker.io/bitnamicharts/elasticsearch -f .\elasticsearch-values.yam

curl -u "elastic:elasticsearch-password" -X GET "http://localhost:9200/_cat/indices?v" 


Deploy Kafka in K8s

helm install kafka oci://registry-1.docker.io/bitnamicharts/kafka


kind load docker-image extended_airflow

# to get password for kafka broker
kubectl get --namespace default secrets/kafka-user-passwords -o jsonpath='{.data.client-passwords}' | base64 --decode

helm install my-release oci://registry-1.docker.io/bitnamicharts/spark

spark-cluster-master-svc 80:80

kubectl port-forward --namespace default svc/spark-cluster-master-svc 80:80

1. Get the Spark master WebUI URL by running these commands:

  kubectl port-forward --namespace default svc/spark-cluster-master-svc 80:80
  echo "Visit http://127.0.0.1:80 to use your application"

2. Submit an application to the cluster:

  To submit an application to the cluster the spark-submit script must be used. That script can be
  obtained at https://github.com/apache/spark/tree/master/bin. Also you can use kubectl run.

  export EXAMPLE_JAR=$(kubectl exec -ti --namespace default spark-cluster-worker-0 -- find examples/jars/ -name 'spark-example*\.jar' | tr -d '\r')

  kubectl exec -ti --namespace default spark-cluster-worker-0 -- spark-submit --master spark://spark-cluster-master-svc:7077 \
    --class org.apache.spark.examples.SparkPi \
    $EXAMPLE_JAR 5

** IMPORTANT: When submit an application from outside the cluster service type should be set to the NodePort or LoadBalancer. **

** IMPORTANT: When submit an application the --master parameter should be set to the service IP, if not, the application will not resolve the master. **




WARNING: There are "resources" sections in the chart not set. Using "resourcesPreset" is not recommended for production. For production installations, please set the following values according to your workload needs:
  - master.resources
  - worker.resources
+info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/


kubectl cp ./dags/scripts/process_weather_data.py spark-cluster-master-0:/opt/bitnami/spark/spark_jobs/process_weather_data.py